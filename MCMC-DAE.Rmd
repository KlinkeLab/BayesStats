---
title: "Markov Chain Monte Carlo Regression of ODE Model"
author: "David Klinke"
date: "`r Sys.Date()`"
output: html_document
bibliography: MCMC_references.bib
csl: nature-no-superscript.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

## Markov Chain Monte Carlo applied to parameter estimation of dynamic model

**Learning objectives**
+ Develop a likelihood function that uses a ODE/DAE solver to generate model predictions.
+ Compare different likelihood relationships that have been proposed in the literature.
+ Motivate convergence criteria applied to the model predictions using an example with embedded kinetic control


To illustrate how Markov Chain Monte Carlo (MCMC) methods can be used to estimate the posterior distribution in the model predictions, we will apply a MCMC approach based on the Metropolis-Hastings algorithm to a simple enzyme kinetics example that is the basis for the Michaelis-Menton relationship. The enzymatic reaction mechanism is comprised of three reactions:
\begin{equation}
E + S \Leftrightarrow^{k_f}_{k_r} C \rightarrow_{k_{cat}} E + P,
\end{equation}
where the substrate ($S$) is converted to a product ($P$) by an enzyme and $E$ and $C$ correspond to the free enzyme and of an enzyme-substrate complex, respectively. The rate constants, $k_f$ (in units of mM$^{-1}$ sec$^{-1}$) and $k_r$ (in units of sec$^{-1}$), quantify the frequency of association of the substrate with a free enzyme and of unproductive dissociation of the enzyme-substrate complex, respectively. The catalytic rate constant, $k_{cat}$ (in units of sec$^{-1}$), quantifies the frequency that enzyme-substrate complexes are able to traverse an activation energy barrier to transform the substrate into product. These three reactions can be expressed in terms of a dynamical system that involves the solution of two ordinary differential equations (ODE) and one algebraic equation. This differential-algebraic system describes the net rate of change in the concentration of substrate,
\begin{equation}
\frac{dS(t)}{dt} = - k_f \cdot E(t) \cdot S(t) + k_r \cdot C(t),
\end{equation}
the net rate of change in the concentration of substrate-enzyme complex,
\begin{equation}
\frac{dC(t)}{dt} = k_f \cdot E(t) \cdot S(t) - (k_r + k_{cat})\cdot C(t).
\end{equation}
and an algebraic relation for the conservation of total enzyme ($E_o$),
\begin{equation}
E_o = E(t) + C(t).
\end{equation}
Together with a non-zero initial values for the substrate concentration ($S(t=0)$) and total enzyme ($E_o$) and values for the three rate constants ($k_f$, $k_r$, and $k_{cat}$), this simple mechanistic model can be used to predict time-dependent changes in the substrate and enzyme-substrate complex. 

To demonstrate the approach, synthetic data was generated by simulating two time-course experiments using the dynamical system and different initial substrate concentrations. We assumed that observing the dynamical system introduces random noise with a root mean square (RMS) deviation equal to 10\% of the maximum substrate concentration. We also assumed values for the total enzyme concentration and three rate constants to be $10^{-5}$ mM ($E_o$), $10^2$ mM$^{-1}$ sec$^{-1}$ ($k_f$), $10^{-1}$ sec$^{-1}$ ($k_r$), and $10^{-2}$ sec$^{-1}$ ($k_{cat}$). The simulated experimental design included two different initial substrate concentrations (Low: 10$^{-3}$ mM and High: $3\times10^{-3}$ mM) sampled at 7 time points. Using the data from this simulated experiment, we used the dynamical system to estimate the posterior distribution in the model predictions using a MCMC method based on the M-H algorithm described in Klinke [@Klinke2009].  MCMC integration was performed in logarithmic parameter space such that all parameters are positive. Uniform priors were used and, for computational efficiency, the prior was bounded between $10^{-8}$ and $10^{8}$. 

First, let's load some libraries behind the scene. In particular, we will use the deSolve package for the function that can solve a system of differential-algebraic equations and Bolstad2 library for the Gelman-Rubin statistic. Then let's create some functions that provide model predictions, given the proposed model parameters.
```{r Preliminaries, echo= TRUE, message = FALSE}
library(deSolve)
library(Bolstad2)

DAE_MM <- function(t, y, ydot, p){
  ## initialize the derivative vector
  res <- vector(mode = "numeric", length = length(y))
  
  ## unpack parameter vector
  ##========================================================================
  kf <- p[1]   # forward rate constant in mM^-1
  kr <- p[2]   # reverse rate constant in sec^-1
  kc <- p[3]   # enzyme catalytic rate constant in sec^-1
  Eo <- p[4]   # Enzyme concentration in mM
  
  ## Relabel species names
  Cl <- y[1] # 1 low Complex
  Sl <- y[2] # 2 low Substrate
  FEl <- y[3] # 3 low free enzyme 
  Ch <- y[4] # 3 High Complex
  Sh <- y[5] # 4 High Substrate
  FEh <- y[6] # 3 low free enzyme 
  
  # ODE Complex - low
  res[1] = kf * FEl * Sl - (kr + kc) * Cl - ydot[1]
  # ODE Substrate - low        
  res[2] = -kf * FEl * Sl + kr * Cl - ydot[2]
  # AE Free enzyme - low
  res[3] = FEl + Cl - Eo
  # Complex - high
  res[4] = kf * FEh * Sh - (kr + kc) * Ch - ydot[4] 
  # Substrate - high        
  res[5] = -kf * FEh * Sh + kr * Ch - ydot[5]
  # AE Free enzyme - high
  res[6] = FEh + Ch - Eo
  
  return(list(res))
}
```

While the MCMC is searching over parameter space that includes initial concentrations and rate parameters, let's create a function that distributes the values in the proposed MCMC step to the corresponding values in the simulation.
```{r assign parameter and IC values}
#Define data points
loadParams <- function(theta){
  # Distribute parameters associated with Initial conditions:  
  y0 <- rep(0, 6)
  y0[1] <- 0           # Low Complex (mM)
  y0[2] <- 10^theta[1] # Low Substrate (mM)
  y0[3] <- 10^(-5)     # Low Free enzyme (mM)
  y0[4] <- 0           # High Complex (mM)
  y0[5] <- 10^theta[2] # High Substrate (mM)
  y0[6] <- 10^(-5)     # High Free enzyme (mM)
  
  # Distribute parameters associated with kinetic rate constants:
  params <- rep(0, 4)
  params[1] <- 10^theta[3]  # kf
  params[2] <- 10^theta[4]  # kr
  params[3] <- 10^theta[5]  # kcat
  params[4] <- 10^(-5)      # Total Enzyme Eo mM
  
  # Estimate initial values of derivatives
  yp <- rep(0, 6)
  # ODE Complex - low
  yp[1] = params[1] * y0[3] * y0[2]
  # ODE Substrate - low        
  yp[2] = -params[1] * y0[3] * y0[2]
  # AE Free enzyme - low
  yp[3] = 0
  # Complex - high
  yp[4] = params[1] * y0[3] * y0[5]
  # Substrate - high        
  yp[5] = -params[1] * y0[3] * y0[5]
  # AE Free enzyme - high
  yp[6] = 0
  
  res <- list(Y0 = y0, YP = yp, Params = params)
  
  return(res)
}
```

Now enter the experimental data. Having multiple replicates at each time point helps ensure that the different between the model and data is not zero but approaches the experimental variance ($\sigma^2$).
```{r enter data}
#Define data points

times <- c(0, 6624, 13606, 23606, 33606, 43606, 50000)
y1r1 <- c(1.08E-03, 4.39E-04, 6.21E-04, 2.22E-04, 1.81E-04, 0.00E+00, 0.00E+00)
y1r2 <- c(8.73E-04, 8.00E-04, 3.32E-04, 4.34E-04, 1.57E-04, 9.61E-05, 7.35E-05)
y1r3 <- c(9.19E-04, 8.14E-04, 3.87E-04, 4.60E-04, 2.07E-04, 2.01E-05, 1.70E-04)
y2r1 <- c(3.00E-03, 2.44E-03, 2.85E-03, 1.94E-03, 1.33E-03, 2.97E-04, 8.83E-05)
y2r2 <- c(2.83E-03, 2.84E-03, 2.08E-03, 1.85E-03, 1.61E-03, 7.91E-04, 9.55E-04)
y2r3 <- c(2.95E-03, 2.25E-03, 2.19E-03, 1.29E-03, 5.22E-04, 4.31E-04, 1.93E-04)
data1 <- data.frame(times = times, y1r1 = y1r1, y1r2 = y1r2, y1r3 = y1r3, 
                    y2r1 = y2r1, y2r2 = y2r2, y2r3 = y2r3)
```

Let's look at the data that consist of a time point in hours (the independent variable) and a measurement of substrate left, which is the dependent variable. The low substrate experiment is shown in red and the high substrate experiment is shown in blue.
```{r calcs}
#Define data points
plot(data1$times/3600, data1$y1r1*10^3, ylim = c(0, 4), xlab = "Time (hours)", ylab = "Substrate (microM)", ty = "p", col = "red")
points(data1$times/3600, data1$y1r2*10^3, col = "red")
points(data1$times/3600, data1$y1r3*10^3, col = "red")
points(data1$times/3600, data1$y2r1*10^3, col = "blue")
points(data1$times/3600, data1$y2r2*10^3, col = "blue")
points(data1$times/3600, data1$y2r3*10^3, col = "blue")
```

Before we create a function for the likelihood evaluation, let's talk a bit about different functions for the likelihood that have been proposed in the literature. Recall that an expression for the likelihood can be obtained by assuming that the errors associated with each experiment are independent samples from multivariate normal distribution with a mean of zero:
\begin{equation}
P(Y|\Theta, M, \Sigma) = \frac{1}{(2\pi)^{\frac{1}{2}N_{obs}k} \cdot |\Sigma|^{\frac{N_{obs}}{2}}} \cdot exp\left[-\frac{1}{2}\cdot\left(v^T \cdot \Sigma^{-1} \cdot v\right)\right]\label{Eqn:4a}
\end{equation}
where $v$ is the the dispersion matrix, $k$ is the number of experimentally observed variables acquired $N_{obs}$ times, and $\Sigma$ is the variance-covariance matrix of the error associated with the experimental observations. The dispersion matrix is a $k \times k$ positive-definite matrix where each element,
\begin{equation}
v_{ij}= \sum_{u=1}^{N_{obs}}\left\{Y_{iu} - M_i(\Theta)\right\}\cdot\left\{Y_{ju} - M_j(\Theta)\right\},\label{Eqn:4}
\end{equation}
corresponds to the normalized sum of the product of the deviation between specific observations, $Y_{iu}$, and their respective model prediction, $M_i(\Theta)$. A common approach for evaluating the likelihood function in Bayesian inference problems is to assume that the variance-covariance matrix of the error associated with the experimental observations (i.e., $\Sigma$) is a scaled identity matrix (i.e., the matrix is diagonal with all non-zero elements identical). However, the error models associated with an observation may depend highly on the experimental assay selected [@KlinkeBiophysJ2008]. In addition, the data used in calibrating the model may correspond to different experimental studies and different techniques. Given that $\Sigma$ is unknown \emph{a priori} in most practical problems, Box and Draper noted that the marginal conditional probability, $P(Y|\Theta,M)$, can be obtained by assuming a prior for $P(\Sigma)$ equal to $|\Sigma|^{-(k+1)/2}$ [@2377-Box1965]. The corresponding likelihood relationship, of the form of a Wishart distribution, can be reduced analytically to a simple form expressed in terms of the determinant of the dispersion matrix:
\begin{equation}
P(Y|\Theta,M) \propto |v|^{-N_{obs}/2}.
\end{equation}
The number of observations now appears in the exponent. In practice, $Y_{iu}$'s are normalized such that repeated measurement using different experimental assays provide normally distributed errors. The dispersion matrix can also be normalized to a reference quantity to minimize the contribution of round-off error in calculating $P(Y|\Theta,M)$. On page 552, Wilks[@2445-Wilks1962] writes that the analytical solution to a Wishart distribution is similar to a likelihood function developed by Mahalanobis, Bose, and Roy using geometric arguments [@Mahalanobis1937], which is equivalent to the determinant of the dispersion matrix. In describing an approximate Bayesian computation (ABC) method, Toni et al. use an approximate Bayesian likelihood function that is described as a Euclidian distance metric, which is equal to the determinant of the dispersion matrix alone and neglects the number of observations associated with the data set [@2447-Toni2009]. 

Each of these different likelihood functions increase as the summed squared error decreased but they have different dependencies. Assuming that $\sigma^2$ is equal to 1 and 10 experimental data points, we can plot different likelihood functions with a Gaussian-based likelihood in black, Wishart distribution likelihood in red, and approximate Bayesian likelihood in blue:
```{r alternative likelihoods}
SSEvalues <- seq(0, 10, by = 0.1)
n = 10
plot(SSEvalues, log((10^SSEvalues)^(-n/2)), type = "l", xlab = "SSE value", ylab = "log10 likelihood value", col = "red") # Wishart distribution result
lines(SSEvalues, log(1/(10^SSEvalues)), col = "blue") # Approximate Bayesian computation
lines(SSEvalues, log(exp(-1*(10^SSEvalues))), col = "black")

```

In the Wishart distribution likelihood, the line becomes steeper as $n$ increases. 

Let's now set up the likelihood evaluation using the likelihood based on the Wishart distribution:
```{r define likelihood function}
LLH <- function(theta, data){
    stepi <- loadParams(theta)

    ## Solving the DAEs using function from deSolve package
    Yhat <- daspk(y = stepi$Y0, dy = stepi$YP, times = data$times, res = DAE_MM, parms = stepi$Params, atol = 1e-10, rtol = 1e-10)
    SSE1 <- sum(((Yhat[,3] - data$y1r1)/mean(data$y1r1))^2)
    SSE2 <- sum(((Yhat[,3] - data$y1r2)/mean(data$y1r2))^2)
    SSE3 <- sum(((Yhat[,3] - data$y1r3)/mean(data$y1r3))^2)
    SSE4 <- sum(((Yhat[,6] - data$y2r1)/mean(data$y2r1))^2)
    SSE5 <- sum(((Yhat[,6] - data$y2r2)/mean(data$y2r2))^2)
    SSE6 <- sum(((Yhat[,6] - data$y2r3)/mean(data$y2r3))^2)
    SSE <- SSE1 + SSE2 + SSE3 + SSE4 + SSE5 + SSE6
    ni <- 6*length(data$y1r1)
    sigma <- 1
#    like <- (1/(2 * pi * sigma^2)^ni) * exp(- SSE/(2*sigma^2))
    like <- SSE^(-ni/2)
    return(like)
}
```

Next we will define the MCMC function. We will add some lines to save the MCMC steps as they are generated so that we can go back later and analyze them.
```{r MCMC function}
#Define Metropolis-Hastings algorithm

MHmcmc <- function(sigma, likelihood, data, steps = 1000, target = 0.2, randomSeed = NULL, startValue = NULL, filename = "Chain1.csv") 
{
  if (steps < 100) {
    warning("Function should take at least 100 steps")
  }
  #determine number of parameter dimensions
  np <- length(sigma)
  if (any(sigma <= 0)) 
    stop("All standard deviations must be strictly non-zero and positive")
  # save the parameter values in the Markov Chain, the scale factor, 
  # and the likelihood evaluation
  targetSample <- matrix(rep(0, (np+2)*steps), nrow = steps, byrow = TRUE)
  
  if (!is.null(randomSeed)) 
    set.seed(randomSeed)
  z <- rnorm(steps, 0, sigma[1])
  for (n in 2:np){
    z <- cbind(z, rnorm(steps, 0, sigma[n]))
  }
  u <- runif(steps)
  if (is.null(startValue)) 
    startValue <- z[1,]
  
  i1 <- 1
  nstep = 1
  accept = 1
  af <- accept/nstep
  Pscale = 1
  
  g <- rep(0, steps)
  proposal <- matrix(rep(0, np*steps), nrow = steps, byrow = TRUE)
  alpha <- rep(0, steps)
  
  g[1] <- likelihood(startValue, data)
  
  targetSample[1,] <- c(startValue, af, g[1])
  
  cat(c("Step", rep("Param", length(startValue)), "AcceptFraction", "Likelihood", "Scale\n"), sep = ",", file = filename)
  cat(c(1, startValue, af, g[1], Pscale, "\n"), sep = ",", file = filename, append = TRUE)
  
  for (n in 2:steps) {
    proposal[n,] <- targetSample[i1,c(1:np)] + Pscale * z[n,]
    # need to include a check to make sure that proposed steps don't go outside of 10^-8 and 10^8
    proposal[n,which(proposal[n,] > 8)] <- 8
    proposal[n,which(proposal[n,] < -8)] <- -8
    
    g[n] <- likelihood(proposal[n,], data)
    k3 <- g[n]
    k4 <- g[i1]
    alpha[n] <- ifelse(k3/k4 > 1, 1, k3/k4)
    if (u[n] >= alpha[n]) {
      targetSample[n,] <- targetSample[i1,]
    }
    else {
      targetSample[n,] <- c(proposal[n,], af, g[n])
      i1 <- n
      accept <- accept + 1
    }
    cat(c(n, targetSample[n,], Pscale, "\n"), sep = ",", file = filename, append = TRUE)
    
    if (nstep >= 200){
      af <- accept/nstep
      if (af > target){
        Pscale <- Pscale * 1.1
      } else if (af < target){
        Pscale <- Pscale * 0.9
      }
      nstep = 0
      accept = 0
    } else {
      nstep = nstep + 1
    }
  }
  return(targetSample)
}
```

Initially, we will use the same proposed step sizes for the five different parameters, but we could change this with experience.

```{r set up MCMC}
#Set up standard deviation of proposal distribution
start <- c(-3, -2.5, 2, -1, -2)
Sig <- c(0.05, 0.05, 0.05, 0.05, 0.05)

# Test to see that the likelihood calculation works
show(LLH(start, data1))
```

Run first MCMC
```{r do MCMC}
#Run first MCMC

TNC1 <- MHmcmc(Sig, LLH, data1, steps = 15000, target = 0.2, startValue = start, filename = "Chain1.csv")
```

Do the MCMC again for three additional chains with different start points.
```{r do MCMC second time}
TNC2 <- MHmcmc(Sig, LLH, data1, steps = 15000, target = 0.2, startValue = c(runif(1) - 3.5,runif(1) - 3, runif(1)+1, runif(1)-1, -2), filename = "Chain2.csv")
TNC3 <- MHmcmc(Sig, LLH, data1, steps = 15000, target = 0.2, startValue = c(runif(1) - 3.5,runif(1) - 3, runif(1)+1, runif(1)-1, -2), filename = "Chain3.csv")
TNC4 <- MHmcmc(Sig, LLH, data1, steps = 15000, target = 0.2, startValue = c(runif(1) - 3.5,runif(1) - 3, runif(1)+1, runif(1)-1, -2), filename = "Chain4.csv")
```

Now using the saved chains, let's plot the traces of the acceptance fraction for each chain.
```{r scale results}
plot(seq(1,nrow(TNC1)), TNC1[,6], ty = "l", ylab = "Acceptance fraction", ylim = c(0,1), xlab = "MCMC step", col = "red")
lines(seq(1,nrow(TNC2)), TNC2[,6], col = "blue")
lines(seq(1,nrow(TNC3)), TNC3[,6], col = "green")
lines(seq(1,nrow(TNC4)), TNC4[,6], col = "orange")
```

Next, let's plot the trace of the four chains for each parameter.
```{r traces}
par(mfrow = c(1, 2), pty = "s")
MCstep <- seq(1, length(TNC1[,1]))
plabel <- c("Low So", "High So", "kf", "kr", "kcat")
for (i in 1:length(plabel)){
  ymin = -8
  ymax = 8
  
  plot(MCstep, TNC1[,i], ylim = c(ymin, ymax), type = "l", xlab = "MCMC step", ylab = "log10 theta", main = plabel[i], col = "red")
  lines(MCstep, TNC2[,i], col = "blue")
  lines(MCstep, TNC3[,i], col = "green")
  lines(MCstep, TNC4[,i], col = "orange")
}
```

### Discussion questions
+ What do you notice?

+ Might you adjust the proposal distribution for the different parameters?

+ Considering what the Gelman-Rubin potential scale reduction factor is calculating, what do you think about the convergence of the parameters?

Let's look at some pairwise scatter plots
```{r scatter plot 1}
library(lattice)

# Collect chains together
MCsteps <- length(TNC1[,1])
TTR <- rbind(TNC1[2000:MCsteps,], TNC2[2000:MCsteps,], TNC3[2000:MCsteps,], TNC4[2000:MCsteps,])

##Pairwise scatter plot for posterior distribution in model parameters
YlOrBr <- c("#0080ff", "#ff0000", "#ffff00")
colori <- colorRampPalette(YlOrBr)

splom(TTR[,c(3:5)], smooth = FALSE, axis.text.cex = 1.0, pscales = 0, 
    varname.cex = 1.5, varname = c("kf", "kr", "kcat"),
    prepanel.limits = function(x) c(-8.0,8.0),
    lower.panel = function(x, y, col, ...) 
    {
        cols <- densCols(x, y, nbin=128, colramp = colori)
        panel.xyplot(x, y, col = cols, pch = ".",   ...)
    },
    upper.panel = function(x, y, ...)
    {
            r <- abs(cor(x, y))
            txt <- format(c(r, 0.123456789), digits=2)[1]
            txt <- paste("", txt, sep="")
        panel.text(0, 0, txt, cex = 2.0 * r)
    }
)
```

Another pairwise scatter plot for posterior distribution in model parameters

```{r pairwise scatter 2}
panel.smth <- function(x, y, col, ...) 
{
    cols <- densCols(x, y, nbin=128, colramp = colori)
    points(x, y, col = cols, pch = ".")
}

panel.cor <- function(x, y, digits=2, prefix="", cex.cor)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits=digits)[1]
    txt <- paste(prefix, txt, sep="")
    if(missing(cex.cor)) cex <- 0.8/strwidth(txt)
    
    text(0.5, 0.5, txt, cex = cex * r)
}
pairs(TTR[,c(3:5)], labels = c("kf", "kr", "kcat"), lower.panel=panel.smth, upper.panel=panel.cor)
```

Given the dynamics of the system and the pseudo-steady-state assumption used to derive a Michaelis-Menton relation, it is not surprising that the kf and kr parameters don't appear to be converged over these chain lengths. From the pair-wise scatter plots, it seems that the data constrains the ratio of these two parameters when the rate of the association between substrate and free enzyme ($k_f$) is still faster than the rate of catalysis. If we get more points to clarify the posterior, we will notice that when the parameter linked to the association reaction becomes slower than the rate of catalysis, the ratio is no longer valid. Overall, the results are consistent with the ideas of rate-limiting or rate-controlling steps and that parameters can have one-sided distributions. Parameters that are associated with reversible reactions that are not rate-controlling are only constrained to be faster than the rate controlling step. If this reversible reaction slows sufficiently, it then becomes the rate-controlling step. 

This example also highlights the point that the convergence criteria, like the Gelman-Rubin potential scale reduction factor, are not only applied to kinetic parameters in the model. Especially in the context of dynamic models, they can also be applied to model predictions. We will talk about that in a future lecture. 

# References
