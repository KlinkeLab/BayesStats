---
title: "Markov Chain Monte Carlo Linear Regression"
author: "David Klinke"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Markov Chain Monte Carlo applied to linear regression

The steps of the Metropolis-Hastings algorithm are:

1. Start at an initial value $\theta^0$ and calculate $f(y|\theta^0)$.

2. Do for $n = 1 ... m$

+ Draw $\theta'$ from $q(\theta^{n-1}, \theta')$ such as: $\theta' = \theta^{n-1} + N(0, 1) \cdot scale$

+ Calculate the probability of acceptance $\alpha(\theta^{n-1}, \theta')$:
\begin{equation}
min\left[1, \frac{f(y|\theta')}{f(y|\theta^{n-1})}\right]
\end{equation}

+ Draw $u$ from $\textrm{Unif}(0, 1)$

+ **If** $u < \alpha(\theta^{n-1}, \theta')$ **then** 
 let $\theta^n = \theta'$ \
 and $\textrm{success} = \textrm{success} + 1$ \
 and increment count.\
**else** let $\theta^n = \theta^{n-1}$ \ 
and increment count.
+ go to next n and periodically update $scale$

Some practical steps that we take to ensure that a Markov Chain is time invariant is that we throw out a certain number of initial steps, which is called the "burn-in" period. In addition, we also start multiple chains at different starting points and we assess how well these chains "mix". Well mixed chains imply that chains that start at different points end up sampling the same state space, that is they are **positive recurrent**.    

So how do you know that chains are sampling the same space? One way to assess this is to use the **Gelman-Rubin potential improvement statistic**. This provides a number that represents the ratio of the variance between chains to the variance within the chains. To do this, we first need to simulate $m$ parallel sequences that each have the same length $n$ (after discarding the "burn-in" start of the chain) and start at different points in parameter space. Starting at different points is referred to as "overdisperse" starting points. For each variable ($\psi$) that we are trying to estimate, we label the simulation daws as $\psi_{ij} (i = 1, ..., n; j = 1, ..., m)$, and we compute the summary statistics $B$ and $W$, which are the between- and within-sequence variances:
\begin{equation}
B = \frac{n}{m-1} \sum_{j = 1}^{m}(\overline{\psi}_{.j} - \overline{\psi}_{..})^2
\end{equation}
where 
\begin{equation}
\overline{\psi}_{.j} = \frac{1}{n} \sum_{i = 1}^{n}\psi_{ij} \textrm{ and }  \overline{\psi}_{..} = \frac{1}{m} \sum_{j = 1}^{m}\overline{\psi}_{.j} 
\end{equation}
and
\begin{equation}
W = \frac{1}{m} \sum_{j = 1}^{m}s_j^2 \textrm{ where } s_j^2 = \frac{1}{n-1} \sum_{i = 1}^{n}(\psi_{ij} - \overline{\psi}_{.j})^2.
\end{equation}
The between-sequence variance, $B$, contains a factor of $n$ because it is based on teh variance of the within-sequence means, $\overline{\psi}_{.j}$, each of which is an average of $n$ values $\psi_{ij}$. If only one sequence is simulated (that is, if $m = 1$), then $B$ cannot be calculated. 

The estimated variance, that is the marginal posterior variance of the estimand, by a weighted average of $W$ and $B$ is given by:
\begin{equation}
\hat{V} = \frac{n - 1}{n}W + \frac{1}{n}B
\end{equation}
This quantity overestimates the posterior variance assuming that the starting distribution is appropriately overdispersed. This becomes an unbiased estimator in the limit of $n \rightarrow \infty$. 

We can then monitor convergence of the iterative simulation by estimating the factor by which the scale of the current distribution for $\psi$ might be reduced if the simulations were continued in the limit of $n \rightarrow \infty$. This potential scale reduction is estimated by:
\begin{equation}
\hat{R} = \sqrt{\frac{\hat{V}}{W}}.
\end{equation}
If the potential scale reduction is high, then we have reason to believe that proceeding with further simulations may improve our inference about the target distribution of the associated scalar estimand. Values of $\hat{R}$ that are less than 1.10 show acceptable convergence. 

A note here, the scalar estimand discussed can be either a parameter value, if the focus is on regression, or a model prediction. Conventionally, the focus is on the model parameters. However depending on how the model is formulated, a converged prediction can be obtained without the underlying model parameters being converged. This can happen when the model has parameters that can not be uniquely identified based on the available data. In processes that exhibit dynamics, the overall dynamics of the system are limited by the slowest steps in the process. Parameters associated with steps that are faster than the rate-limiting steps can exhibit one-sided distributions. 

#### An example
1. Let's do a regression of a linear model to data obtained from two different conditions. We will use a Markov Chain Monte Carlo approach to estimate the posterior distribuition in the model parameters. 

```{r Preliminaries, echo= FALSE, message = FALSE}
library(Bolstad2)
```

Let's load some libraries behind the scene. In particular, we will use the Bolstad2 library for the Gelman-Rubin statistic, although there are other packages that provide this function. Next we will define the MCMC function:

```{r MCMC function}
#Define Metropolis-Hastings algorithm

MHmcmc <- function(sigma, likelihood, data, steps = 1000, target = 0.2, randomSeed = NULL, startValue = NULL) 
{
  if (steps < 100) {
    warning("Function should take at least 100 steps")
  }
  #determine number of parameter dimensions
  np <- length(sigma)
  if (any(sigma <= 0)) 
    stop("All standard deviations must be strictly non-zero and positive")
  # save the parameter values in the Markov Chain, the scale factor, 
  # and the likelihood evaluation
  targetSample <- matrix(rep(0, (np+2)*steps), nrow = steps, byrow = TRUE)
  
  if (!is.null(randomSeed)) 
    set.seed(randomSeed)
  z <- rnorm(steps, 0, sigma[1])
  for (n in 2:np){
    z <- cbind(z, rnorm(steps, 0, sigma[n]))
  }
  u <- runif(steps)
  if (is.null(startValue)) 
    startValue <- z[1,]
  
  i1 <- 1
  nstep = 1
  accept = 1
  af <- accept/nstep
  
  g <- rep(0, steps)
  proposal <- matrix(rep(0, np*steps), nrow = steps, byrow = TRUE)
  alpha <- rep(0, steps)

  g[1] <- likelihood(startValue, data)

  targetSample[1,] <- c(startValue, af, g[1])
  
  for (n in 2:steps) {
    proposal[n,] <- targetSample[i1,c(1:np)] + z[n,]
    g[n] <- likelihood(proposal[n,], data)
    k3 <- g[n]
    k4 <- g[i1]
    alpha[n] <- ifelse(k3/k4 > 1, 1, k3/k4)
    if (u[n] >= alpha[n]) {
      targetSample[n,] <- targetSample[i1,]
    }
    else {
      targetSample[n,] <- c(proposal[n,], af, g[n])
      i1 <- n
      accept <- accept + 1
    }
    if (nstep >= 200){
      af <- accept/nstep
      if (af > target){
        z <- z * 1.1
      } else if (af < target){
        z <- z * 0.9
      }
      nstep = 0
      accept = 0
    } else {
      nstep = nstep + 1
    }
  }
  return(targetSample)
}
```

Define the data points, which comprise a time point in hours and a measurement, which is the dependent variable:

```{r calcs}
#Define data points
NC <- matrix(c(2,  0.988850026,  2,	1.021569338, 2,	0.989580636, 4,	0.853398211, 4,	0.853895625, 4,	1.28523392, 4,	1.007472244, 
               10,	1.262924495, 10,	0.997538414, 10,	1.105481178, 10,	0.731030243, 10,	0.90302567), nrow = 12, byrow = TRUE)

P75 <- matrix(c(2,  2.256871955, 2,	2.45309514, 2,	2.560123986, 4,	2.949653866, 4,	3.384441969, 4,	2.475052574, 4,	2.892547513, 4,	1.904991892, 
                10,	3.785547475, 10,	5.079586603, 10,	4.495100205, 10,	4.084163797, 10,	3.413358253, 10,	3.922278872), nrow = 14, byrow = TRUE)

plot(NC[,1], NC[,2], ylim = c(0.5, 6), ty = "p", col = "red")
points(P75[,1], P75[,2], col = "blue")
```

Let's now set up the likelihood evaluation:
```{r likelihood}
#Likelihood evaluation
LLH <- function(theta, data){
  Yhat <- theta[1] * data[,1] + theta[2]
  SSE <- sum((data[,2] - Yhat)^2)
  ni <- length(data)
  sigma <- 1
  like <- (1/(2 * pi * sigma^2)^ni) * exp(- SSE/(2*sigma^2))
  return(like)
}
```

Here we are going to use different proposed step sizes for the two different parameters.

```{r do MCMC}
#Set up standard deviation of proposal distribution
Sig <- c(0.1, 0.5)

TNC1 <- MHmcmc(Sig, LLH, NC, steps = 4000, target = 0.2, startValue = c(0,1))
TNC2 <- MHmcmc(Sig, LLH, NC, steps = 4000, target = 0.2, startValue = c(runif(1),runif(1)))
TNC3 <- MHmcmc(Sig, LLH, NC, steps = 4000, target = 0.2, startValue = c(runif(1),runif(1)))
TNC4 <- MHmcmc(Sig, LLH, NC, steps = 4000, target = 0.2, startValue = c(runif(1),runif(1)))
```

As Bayesian data analysis is not widely used, there are a number of aspects that you need to show when reporting the results of a Markov Chain Monte Carlo simulation (from Kruschke - Doing Bayesian Data Analysis. 

1. **Motivate the use of a Bayesian analysis:** Many audiences are unfamiliar with Bayesian methods so some motivation as to why you are using this approach may be helpful. 

2. **Clearly describe the data structure, the model, and the model’s parameters:** Ultimately you want to report the meaningful parameter values, but you can’t do that until you explain the model, and you can’t do that until you explain the data being modeled. Therefore, recapitulate the data structure, reminding your reader of the predicted and predictor variables. Then describe the model, emphasizing the meaning of the parameters. This task of describing the model can be arduous for complex hierarchical models, but it is necessary and crucial if your analysis is to mean anything to your audience.

3. **Clearly describe and justify the prior:** It is important to convince your audience that your prior is appropriate and does not predetermine the outcome. The prior should be amenable to a skeptical audience. 

4. **Report the MCMC details**, especially evidence that the chains were converged and of sufficient length. Typically one presents a trace of the different chains to show that they are randomly traversing parameter space. In addition, one should show a metric of convergence as a function of chain length. Once the chain has converged, the subsequent samples represent random samples from the posterior. Those samples should be used for inference. 

5. **Interpret the posterior:** Many models have dozens or even hundreds of parameters, and therefore it is impossible to summarize all of them. The choice of which parameters or contrasts to report is driven by domain-specific theory and by the results themselves. You want to report the parameters and contrasts that are theoretically meaningful. You can report the posterior central tendency of a parameter and its HDI in text alone; histograms of posteriors are useful for the analyst to understand the posterior and for explanation, but may be unnecessary in a concise report. Bivariate scatter plots can be helpful to determine whether certain parameters are correlated and whether they exhibit non-Gaussian distribution. 

Here let's plot the trace of the acceptance fraction of the first chain that was used to regress condition 1.
```{r scale results}
plot(seq(1,nrow(TNC1)), TNC1[,3], ty = "l", ylab = "Acceptance fraction", xlab = "MCMC step", col = "red")
```

Now let's plot the trace of the four chains that were used to regress condition 1.
```{r results}
par(mfrow = c(2, 2), pty = "s")
plot(TNC1[,1], TNC1[,2], xlim = c(-0.5, 0.5), ylim = c(-1.0,3.0), ty = "l", col = "red", main = "Chain 1")
plot(TNC2[,1], TNC2[,2], xlim = c(-0.5, 0.5), ylim = c(-1.0,3.0), ty = "l", col = "blue", main = "Chain 2")
plot(TNC3[,1], TNC3[,2], xlim = c(-0.5, 0.5), ylim = c(-1.0,3.0), ty = "l", col = "green", main = "Chain 3")
plot(TNC4[,1], TNC4[,2], xlim = c(-0.5, 0.5), ylim = c(-1.0,3.0), ty = "l", col = "orange", main = "Chain 4")
#lines(TNC2[,1], TNC2[,2], col = "blue")
#lines(TNC3[,1], TNC3[,2], col = "green")
#lines(TNC4[,1], TNC4[,2], col = "orange")
```

Let's calculate the Gelman-Rubin statistic on the parameters regressed to first condition.

```{r Gelman-Rubin stat, message = FALSE}
# Use Gelman-Rubin potential improvement statistic
# Ratio of variance between chains / variance within chain
#

slope <- cbind(TNC1[,1], TNC2[,1], TNC3[,1], TNC4[,1])
int <- cbind(TNC1[,2], TNC2[,2], TNC3[,2], TNC4[,2])
Xval <- seq(100, nrow(TNC1), by = 100)
mGR <- rep(0, length(Xval))
bGR <- rep(0, length(Xval))
for (i in 1:length(Xval)){
  tmp <- GelmanRubin(slope[1:Xval[i],])
  mGR[i] <- tmp$R
  tmp <- GelmanRubin(int[1:Xval[i],])
  bGR[i] <- tmp$R
}
```

and plot the resulting metric.

```{r plot Gelman-Rubin stat, message = FALSE}
# Use Gelman-Rubin potential improvement statistic
# Ratio of variance between chains / variance within chain
#
plot(Xval, mGR, ty = "l", xlim = c(0,4000), ylab = "Gelman-Rubin PSRF", xlab = "MCMC step", col = "blue")
lines(Xval, bGR, col = "red")
```

Plot traces
```{r traces}
MCstep <- seq(1, length(TNC1[,1]))
ymin <- min(c(TNC1[,1], TNC2[,1], TNC3[, 1], TNC4[,1]))
ymax <- max(c(TNC1[,1], TNC2[,1], TNC3[, 1], TNC4[,1]))

plot(MCstep, TNC1[,1], ylim = c(ymin, ymax), type = "l", xlab = "MCMC step", col = "red")
lines(MCstep, TNC2[,1], col = "blue")
lines(MCstep, TNC3[,1], col = "green")
lines(MCstep, TNC4[,1], col = "orange")
```

We can also plot the histograms of the parameter values determined from the converged segments of the chains.

```{r histograms}
h1 <- density(c(TNC1[2000:4000,1], TNC2[2000:4000,1], TNC3[2000:4000,1], TNC4[2000:4000,1]))
ymax <- max(c(h1$y)) * 1.05
plot(h1$x, h1$y, type = "l", col = "black", xlim = range(-0.5,0.5), 
     ylim = c(0, ymax), main = "Posterior Slope", 
     xlab = "slope", ylab = "Density")

h1 <- density(c(TNC1[2000:4000,2], TNC2[2000:4000,2], TNC3[2000:4000,2], TNC4[2000:4000,2]))
ymax <- max(c(h1$y)) * 1.05
plot(h1$x, h1$y, type = "l", col = "black", xlim = range(-2,3), 
     ylim = c(0, ymax), main = "Posterior Intercept", 
     xlab = "intercept", ylab = "Density")
```

Are these two parameters correlated? A scatter biplot may help to visualize. 
```{r 2D scatter plot}
m <- c(TNC1[2000:4000,1], TNC2[2000:4000,1], TNC3[2000:4000,1], TNC4[2000:4000,1])
b <- c(TNC1[2000:4000,2], TNC2[2000:4000,2], TNC3[2000:4000,2], TNC4[2000:4000,2])

colori <- colorRampPalette(c("blue", "red", "yellow"))
cols <- densCols(m, b, nbin = 128, colramp = colori)
plot(m, b, type = "p", xlab = "slope", ylab = "intercept", col = cols) 

```

Now let's regress the second experimental condition using MCMC: 
```{r results 2}
TP751 <- MHmcmc(Sig, LLH, P75, steps = 4000, target = 0.2, startValue = c(0,1))
TP752 <- MHmcmc(Sig, LLH, P75, steps = 4000, target = 0.2, startValue = c(runif(1),runif(1)))
TP753 <- MHmcmc(Sig, LLH, P75, steps = 4000, target = 0.2, startValue = c(runif(1),runif(1)))
TP754 <- MHmcmc(Sig, LLH, P75, steps = 4000, target = 0.2, startValue = c(runif(1),runif(1)))
```


+ Do you still need the same number of steps to converge the posterior in the second example?

+ Are the parameters different between the two conditions?
